{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17459625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dominik Kowalczyk \n",
    "#Lista 1\n",
    "\n",
    "#Zrobić i uzupełnić na podstawie pseudokodu \n",
    "\n",
    "#Zadanie 1 - Regresja liniowa\n",
    "import numpy as np\n",
    "import pytest\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class LinearRegr:\n",
    "    def fit(self, X, Y):\n",
    "        #X = np.array, shape = (n, m)\n",
    "        n, m = X.shape\n",
    "\n",
    "        #Wektor parametrów Beta musi mieć m+1 (mamy m dla liczby X oraz +1 dla wyrazu wolnego beta0)\n",
    "        self.beta = np.zeros((m+1))\n",
    "\n",
    "        #Uwaga: przed zastosowaniem wzoru do X nalezy dopisac kolumne zlozona z jedynek!\n",
    "        X_jedynki = np.hstack((np.ones((n, 1)), X))\n",
    "\n",
    "        #Znajduje beta minimalizujace kwadratowa funkcje kosztu L uzywajac wzoru.\n",
    "        #Wzór z wykładu: beta = (X^T X)^(-1) X^T Y \n",
    "        A = X_jedynki.T @ X_jedynki\n",
    "        b = X_jedynki.T @ Y\n",
    "\n",
    "        #Rozwiązujemy i nadpisujemy wyniki (tam gdzie mamy m+1 zer) -->  A*beta = b\n",
    "        self.beta = np.linalg.solve(A, b)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        #wejscie X = np.array, shape = (k, m)\n",
    "        k, m = X.shape\n",
    "        \n",
    "        #Dodaj kolumnę jedynek do X\n",
    "        X_jedynki = np.hstack((np.ones((k, 1)), X))\n",
    "        \n",
    "        #Wzór na prognoze - macierzowo X (z jedynkami) * beta\n",
    "        Y_prognoza = X_jedynki @ self.beta\n",
    "\n",
    "        #zwraca Y = wektor(f(X_1), ..., f(X_k)\n",
    "        return Y_prognoza\n",
    "\n",
    "\n",
    "def test_RegressionInOneDim():\n",
    "    X = np.array([1,3,2,5]).reshape((4,1))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    a = np.array([1,2,10]).reshape((3,1))\n",
    "    expected = LinearRegression().fit(X, Y).predict(a)\n",
    "    actual = LinearRegr().fit(X, Y).predict(a)\n",
    "    assert list(actual) == pytest.approx(list(expected))\n",
    "\n",
    "def test_RegressionInThreeDim():\n",
    "    X = np.array([1,2,3,5,4,5,4,3,3,3,2,5]).reshape((4,3))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    a = np.array([1,0,0, 0,1,0, 0,0,1, 2,5,7, -2,0,3]).reshape((5,3))\n",
    "    expected = LinearRegression().fit(X, Y).predict(a)\n",
    "    actual = LinearRegr().fit(X, Y).predict(a)\n",
    "    assert list(actual) == pytest.approx(list(expected))\n",
    "\n",
    "test_RegressionInOneDim()               #jak nic nie zwracają to wszystko jest tak jak powinno byc - assert \n",
    "test_RegressionInThreeDim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5784589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54685378 -1.76188321  1.58691716  5.15527388  3.66704391]\n",
      "[ 0.54684663 -1.76188798  1.58691183  5.15528137  3.66704223]\n"
     ]
    }
   ],
   "source": [
    "#Zadanie 2 - iteracja Ridge\n",
    "import numpy as np\n",
    "import pytest\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "class RidgeRegr:\n",
    "    def __init__(self, alpha, zmienna_uczaca=0.01, maks_iteracja=10000, tolerancja=1e-8):\n",
    "        self.alpha = alpha  \n",
    "        self.zmienna_uczaca = zmienna_uczaca        \n",
    "        self.maks_iteracja = maks_iteracja\n",
    "        self.tolerancja = tolerancja\n",
    "\n",
    "        #B = (X^T * X + alfaI)^-1 * X^T *Y\n",
    "    def fit(self, X, Y):\n",
    "        n, m = X.shape\n",
    "        \n",
    "        #Dodaj kolumnę jedynek dla beta_0\n",
    "        X_jedynki1 = np.hstack([np.ones((n, 1)), X])\n",
    "        #tak samo jak w zadaniu 1 \n",
    "        self.beta = np.zeros(m + 1)\n",
    "\n",
    "        #funkcja kosztu L(θ) = (1/2)*Σ_{i=1}^{n} (y_i - f(x_i))^2 + α/2*Σ_{j=1}^{m} Beta^2\n",
    "        #jak policzymy pochodna dostaniemy 1/2 * 2X^T(X*beta - Y)\n",
    "        #gradient mozna podzielic na czesc regularyzacji (alfa/2 suma B^2)\n",
    "        for i in range(self.maks_iteracja):\n",
    "            y_prognoza = X_jedynki1 @ self.beta\n",
    "            roznica = y_prognoza - Y                #minus w gadiencie do środka\n",
    "\n",
    "            #w zeszycie napisane wyprowadzenie \n",
    "            #gradient błędu + regularyzacja (wspolczynniki oprocz beta0 bo to stala - czyli pomijamy 0 - stad [1:])\n",
    "            gradient = (X_jedynki1.T @ roznica) + self.alpha * np.r_[0, self.beta[1:]]\n",
    "\n",
    "            #zapamietujemy stare wagi (aby potencjalnie wrocic)\n",
    "            stara_beta = self.beta.copy()\n",
    "            #bierzemy nowa bete uaktualniona o nasz gradient i zmienna uczaca\n",
    "            self.nowa_beta = self.beta - self.zmienna_uczaca * gradient\n",
    "            #aktualizujemy na nowa\n",
    "            self.beta = self.nowa_beta\n",
    "\n",
    "            #nalezy kiedys przerwac, czyli wtedy gdy nasza minimalizacja funkcji kosztu jest bardzo mala, czyli praktycznie przestala sie uczyc\n",
    "            #sprawdzamy klasyczna norma euklidesowa\n",
    "            if np.linalg.norm(self.beta - stara_beta) < self.tolerancja:\n",
    "                break   \n",
    "\n",
    "        return self\n",
    "    \n",
    "    #funcja predict analogiczna do tej z zadania 1\n",
    "    def predict(self, X):\n",
    "        k, m = X.shape\n",
    "        #dodajemy 1 - czyli aby byl wyraz wolny beta0 (kolumna 1 na poczatku)\n",
    "        X_jedynki2 = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        Y_predykcja = X_jedynki2 @ self.beta\n",
    "        return Y_predykcja\n",
    "\n",
    "def test_RidgeRegressionInOneDim():\n",
    "    X = np.array([1,3,2,5]).reshape((4,1))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    X_test = np.array([1,2,10]).reshape((3,1))\n",
    "    alpha = 0.3\n",
    "    expected = Ridge(alpha).fit(X, Y).predict(X_test)\n",
    "    actual = RidgeRegr(alpha).fit(X, Y).predict(X_test)\n",
    "    assert list(actual) == pytest.approx(list(expected), rel=1e-5)\n",
    "\n",
    "def test_RidgeRegressionInThreeDim():\n",
    "    X = np.array([1,2,3,5,4,5,4,3,3,3,2,5]).reshape((4,3))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    X_test = np.array([1,0,0, 0,1,0, 0,0,1, 2,5,7, -2,0,3]).reshape((5,3))\n",
    "    alpha = 0.4\n",
    "    expected = Ridge(alpha).fit(X, Y).predict(X_test)\n",
    "    actual = RidgeRegr(alpha).fit(X, Y).predict(X_test)\n",
    "    #assert list(actual) == pytest.approx(list(expected), rel=1e-3)\n",
    "    print(expected)\n",
    "    print(actual)\n",
    "test_RidgeRegressionInOneDim()                 \n",
    "test_RidgeRegressionInThreeDim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ef6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
